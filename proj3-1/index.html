<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>
	<style>
		div.padded {
			padding-top: 0px;
			padding-right: 100px;
			padding-bottom: 0.25in;
			padding-left: 100px;
		}
	</style>
	<title>Project 3-1 Lisa Cheung,Queenie Lau| CS 184</title>
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
	<link rel="stylesheet" type="text/css" href="style.css" media="screen" />
</head>

<body>
	<br />
	<h1 align="middle">Assignment 3: PathTracer</h1>
	<h2 align="middle">Lisa Cheung, Queenie Lau</h2>

	<div class="padded">
		<b>
			<p>Overview</p>
		</b>

		<p>
			In this project, I implemented the ray generation, triangle intersection, and sphere intersection
			algorithms.
			Then, using the average of centroids for a heuristic, I implemented the BVH algorithm to speed up image
			renderings.
			I compared the differences in noise between hemisphere sampling and importance sampling, then added indirect
			lighting for global
			illumination. Finally, I enabled adaptive sampling by converging based on the pixel's convergence value and
			compared the difference in sampling rate
			for different regions of the image.
		</p>


		<h2 align="middle">Part 1: Ray Generation and Intersection</h2>
		<ul>
			<li>
				<p>Walk through the ray generation and primitive intersection parts of the rendering pipeline.</p>
			</li>
		</ul>

		<p>To implement ray generation, I first translated the normalized input camera coordinates to camera space
			coordinates by first translating the input x and y coordinates by -0.5, then scaling with
			tan(0.5*radians(hFov)) for x and tan(0.5*radians(vFov)) for y and normalizing.
			I went from the ray in camera space to the ray in world space by using the camera-to-world rotation
			matrix on camera space coordinates and normalizing it to get the direction vector in world space.
			I set the origin for the world space ray to the camera position in world space.
			We used this function for generating camera rays in raytrace_pixel, where we iterated num_samples times
			and in each iteration, we called a uniform random grid sampler between range [0,1] and added it to x and
			y.
			We normalized it by width and height of sampleBuffer. We used est_radiance_global_illuminance then
			updated the pixel in the sample buffer.
			We had primitive intersections with triangles and spheres.
			To test for intersections with the sphere given an input ray, and the min_t and max_t of the ray, I used
			the discriminant.
			If the discriminant was equal to zero, then there was an intersection at one point and if it was greater
			than zero then there was an intersection at two points.
			Otherwise, there is no intersection. I used the quadratic formula to find the min_t and max_t of the
			ray, where at^2 + bt + c = 0, a = dot(r.d, r.d), b = dot(2*(r.o - this->o), r.d), and c
			=dot((r.o-this->o), (r.o - this->o)) - (this->r2).
			If there was an intersection, I checked that the updated t1 and t2 found with the quadratic equations
			were within the range of min_t and max_t.
			If they were I updated the ray’s max_t to be the nearest intersection.
			Also, I updated the Intersection structure, computing the surface normal which is the normalized point
			that intersects the sphere at min(t1,t2).
		</p>

		<ul>
			<li>
				<p>
					Explain the triangle intersection algorithm you implemented in your own words.
				</p>
			</li>
		</ul>

		<p>
			For the triangle intersection algorithm, I tested to see if the ray intersected with the triangle using
			the Moller Trumbore algorithm.
			div align="center">
		<div>
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/mollertrumbore.png" width="480px"
							alt="img srce: https://cs184.eecs.berkeley.edu/sp22/lecture/9-22/ray-tracing" />
						<figcaption align="middle">Moller Trumbore Algorithm</figcaption>
				</tr>
			</table>
		</div>
		I checked whether the resulting t was valid and within the range of the ray’s min_t and max_t, and used
		valid barycentric coordinates for updating the nearest intersection point of the ray and getting the
		surface normal. I updated the intersection structure.

		</p>
		<ul>
			<li>
				<p>
					Show images with normal shading for a few small .dae files
				</p>
			</li>
		</ul>

		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/image19.png" width="480px" />
						<figcaption align="middle"></figcaption>
				</tr>
			</table>
		</div>
		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/image20.png" width="480px" />
						<figcaption align="middle"></figcaption>
				</tr>
			</table>
		</div>
		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/image21.png" width="480px" />
						<figcaption align="middle"></figcaption>
				</tr>
			</table>
		</div>

		<h2 align="middle">Part 2:Bounding Volume Hierarchy</h2>
		<ul>
			<li>
				<p>Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the
					splitting
					point.</p>
			</li>
		</ul>

		<p>For the BVH construction algorithm, I initiated a BBox and expanded it to include all the primitives from
			start to end, and computed the average centroid of all the primitives.
			If the number of primitives was more than max_leaf_size, then we partitioned the left and right
			iterators depending on if the centroid value was less than the average centroid value.

			I chose the axis that was the most evenly split between the left and right nodes.
			Then, I recursed on the left and right nodes. If the number of primitives was less than the
			max_leaf_size, I returned the leaf node.
		</p>
		<ul>
			<li>
				<p>Show images with normal shading for a few large .dae files that you can only render with BVH
					acceleration.</p>
			</li>
		</ul>

		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/image4.png" width="480px" />
						<figcaption align="middle"></figcaption>
				</tr>
			</table>
		</div>
		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/image5.png" width="480px" />
						<figcaption align="middle"></figcaption>
				</tr>
			</table>
		</div>
		<ul>
			<li>
				<p>Compare rendering times on a few scenes with moderately complex geometries with and without BVH
					acceleration. Present your results in a one-paragraph analysis.</p>
			</li>
		</ul>

		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/image11.png" width="480px" />
						<figcaption align="middle"></figcaption>
				</tr>
			</table>
		</div>
		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/image12.png" width="480px" />
						<figcaption align="middle"></figcaption>
				</tr>
			</table>
		</div>
		<p>
			It took 0.0916 seconds to render the cow with bvh acceleration compared to around 40 seconds without BVH
			acceleration.
			It took 0.1661 seconds to render the beetle with BVH acceleration compared to 43 seconds without BVH
			acceleration.
			Using the BVH improved the runtime from linear runtime to log n runtime because we could ignore nodes with
			bounding boxes if there was no ray intersection and traverse through the nodes more quickly.
			We do not have to linearly iterate and test every single primitive and instead only test for intersection
			with the primitive if it first intersects the bounding box.

		</p>
		<h2 align="middle">Part 3:Direct Illumination</h2>
		<ul>
			<li>
				<p>
					Walk through both implementations of the direct lighting function.
				</p>
			</li>
		</ul>

		<p>
			For direct lighting with uniform hemisphere sampling, I iterated through
			scene->lights.size()*ns_area_light samples.
			Since we are doing uniform hemisphere sampling for ray directions, I used the constant 1/(2*PI) as the
			distribution p(w).
			In each iteration, I estimated the outgoing light with the reflection equation.In each iteration, I
			estimated the outgoing light with the reflection equation.
			First, I got a sample direction from the hemisphereSampler and used it to get the sample ray with an
			origin of hit_p.
			I used world space for the ray’s direction.
			If there was an intersection with the sample ray, I solved for the reflection equation using the
			estimator.
			For the estimator, I multiplied together the z-component of the ray’s direction(cos), bsdf, and the
			luminance, then divided by the distribution p(w).
			The resulting value was the average of the samples.
			For direct lighting with importance sampling, I iterated scene->lights times and if it was a delta
			light(point light source), I only sampled once and if it wasn’t I sampled ns_area_light times.
			Using sample_L for the light at hit_p I found the emitted radiance, the probability density function
			value, and sampled direction wi between the light source and p.
			I then created a ray with origin hit_p and direction wi, and if there was an interaction with the sample
			ray, I applied the estimator equation.
			I multiplied together the emitted radiance, bsdf, and cosine, then divided by the probability density
			function value. I then took the average.

		</p>
		<ul>
			<li>
				<p>
					Show some images rendered with both implementations of the direct lighting function.
				</p>
			</li>
		</ul>

		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/image6.png" width="480px" />
						<figcaption align="middle">Hemisphere Sampling</figcaption>
				</tr>
			</table>
		</div>
		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/image9.png" width="480px" />
						<figcaption align="middle">Importance Sampling</figcaption>
				</tr>
			</table>
		</div>
		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/image22.png" width="480px" />
						<figcaption align="middle">Importance Sampling</figcaption>
				</tr>
			</table>
		</div>
		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/pt3_hemisphere_sphere.png" width="480px" />
						<figcaption align="middle">Hemisphere Sampling</figcaption>
				</tr>
			</table>
		</div>
		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/pt3_light_sphere.png" width="480px" />
						<figcaption align="middle">Importance Sampling</figcaption>
				</tr>
			</table>
		</div>
		<ul>
			<li>
				<p>
					Focus on one particular scene with at least one area light and compare the noise levels in soft
					shadows
					when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s
					flag)
					using light sampling, not uniform hemisphere sampling.
				</p>
			</li>
		</ul>

		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/image18.png" width="480px" />
						<figcaption align="middle">1 light ray</figcaption>
				</tr>
			</table>
		</div>
		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/image2.png" width="480px" />
						<figcaption align="middle">4 light ray</figcaption>
				</tr>
			</table>
		</div>
		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/image10.png" width="480px" />
						<figcaption align="middle">16 light ray</figcaption>
				</tr>
			</table>
		</div>
		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/image16.png" width="480px" />
						<figcaption align="middle">64 light ray</figcaption>
				</tr>
			</table>
		</div>
		<ul>
			<li>
				<p>
					Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph
					analysis.
				</p>
			</li>
		</ul>

		<p>
			The results from hemisphere sampling have some more noise and blur compared to the lighting sampling since
			for hemisphere
			sampling we are sampling uniformly from a hemisphere in contrast to lighting sampling in which we
			sample the lights directly. Direct lighting via importance sampling gives better results since we only
			consider sampling
			between the light source and the hit point of the ray directions, then measure the total amount of light
			that exits.
		</p>
		<h2 align="middle">Part 4:Global Illumination</h2>
		<ul>
			<li>
				<p>
					Walk through your implementation of the indirect lighting function.
				</p>
			</li>
		</ul>

		<p>
			To implement the at_least_one_bounce_radiance function, I returned one bounce radiance and extra bounces
			with recursion depending on the Russian Roulette probability and the max_ray_depth. If the max_ray_depth
			was 0, I only returned zero_bounce and if it was 1 I returned only one bounce. My
			at_least_one_bounce_radiance function called the one_bounce_radiance function and depending on whether
			the coin flip with conditional probability of 0.7 was heads or if the ray depth was the max ray depth
			and greater than one, then I checked whether the ray was intersecting and could recurse. I used the
			estimator function which involved calling itself multiplied to bsdf and cosine, divided by the
			probability density function. If it used Russian roulette, I divided by the conditional probability 0.7.
			I summed up the returning results to get the global illumination.
		</p>
		<ul>
			<li>
				<p>
					Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per
					pixel.
				</p>
			</li>
		</ul>

		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/image1.png" width="480px" />
						<figcaption align="middle">Global Illumination</figcaption>
				</tr>
			</table>
		</div>
		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/image14.png" width="480px" />
						<figcaption align="middle">Global Illumination</figcaption>
				</tr>
			</table>
		</div>
		<ul>
			<li>
				<p>
					Pick one scene and compare rendered views first with only direct illumination, then only indirect
					illumination. Use 1024 samples per pixel. (You will have to edit
					PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
				</p>
			</li>
		</ul>

		<p>
			In indirect lighting, we estimate bounces of light with recursion and importance sampling, resulting in
			images where light bounces off of objects's surfaces to another surface in the -w direction and adds
			indirect lighting.Direct lighting, however, only directly lights up the regions where there is a light
			source directly
			pointing at it, and hence looks darker.
		</p>
		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/image7.png" width="480px" />
						<figcaption align="middle">Direct Illumination</figcaption>
				</tr>
			</table>
		</div>
		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/image15.png" width="480px" />
						<figcaption align="middle">Indirect Illumination</figcaption>
				</tr>
			</table>
		</div>
		<p>
			To render indirect illumination, I subtracted one_bounce_radiance from the recursive call
			at_least_one_bounce since indirect illumination doesn’t involved one_bounce or zero_bounce. For direct
			illumination, I left out the recursive calls at_least_one_bounce and only included one_bounce and
			zero_bounce.
		</p>
		<ul>
			<li>
				<p>
					For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag).
					Use
					1024 samples per pixel.
				</p>
			</li>
		</ul>

		<p>
			There is no indirect lighting when max_ray_depth is 1, and only occurs when max_ray_depth is greater than 1.
			For max_ray_depth of 1, there is only direct lighting. For max_ray_depth of 2 and 3, the images are brighter
			since
			it accounts for indirect bounces of light. It is hard to tell much difference at max_ray_depth of 100;
			however, there is slight more light around the wall surfaces.
		</p>
		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/image13.png" width="480px" />
						<figcaption align="middle">max_ray_depth=0</figcaption>
				</tr>
			</table>
		</div>
		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/image17.png" width="480px" />
						<figcaption align="middle">max_ray_depth=1</figcaption>
				</tr>
			</table>
		</div>
		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/image3.png" width="480px" />
						<figcaption align="middle">max_ray_depth=2</figcaption>
				</tr>
			</table>
		</div>
		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/image1.png" width="480px" />
						<figcaption align="middle">max_ray_depth=3</figcaption>
				</tr>
			</table>
		</div>
		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/image8.png" width="480px" />
						<figcaption align="middle">max_ray_depth=100</figcaption>
				</tr>
			</table>
		</div>
		<ul>
			<li>
				<p>
					Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1,
					2,
					4, 8, 16, 64, and 1024. Use 4 light rays.
				</p>
			</li>
		</ul>

		<p>As the sample-per-pixel rate increases, the images have less noisy. Increasing the number of samples over a
			pixel area increases the accuracy for estimating the radiance. </p>
		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/image41.png" width="480px" />
						<figcaption align="middle">sample-per-pixel rate=1</figcaption>
				</tr>
			</table>
		</div>
		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/image42.png" width="480px" />
						<figcaption align="middle">sample-per-pixel rate=2</figcaption>
				</tr>
			</table>
		</div>
		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/image43.png" width="480px" />
						<figcaption align="middle">sample-per-pixel rate=4</figcaption>
				</tr>
			</table>
		</div>
		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/image44.png" width="480px" />
						<figcaption align="middle">sample-per-pixel rate=8</figcaption>
				</tr>
			</table>
		</div>
		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/image45.png" width="480px" />
						<figcaption align="middle">sample-per-pixel rate=16</figcaption>
				</tr>
			</table>
		</div>
		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/image46.png" width="480px" />
						<figcaption align="middle">sample-per-pixel rate=64</figcaption>
				</tr>
			</table>
		</div>
		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/image47.png" width="480px" />
						<figcaption align="middle">sample-per-pixel rate=1024</figcaption>
				</tr>
			</table>
		</div>
		<h2 align="middle">Part 5:Adaptive Sampling</h2>
		<ul>
			<li>
				<p>
					Walk through your implementation of adaptive sampling.
				</p>
			</li>
		</ul>


		<p>
			To add adaptive sampling, I measured the pixel’s convergence per every interval of batch samples. During
			each iteration, I added the samples illuminance and illuminance**2 into variables s1 and s2. I found the
			mean, variance, and pixel’s convergence value with the formulas found in the spec. If the pixel’s
			convergence value was less than maxTolerance*mean, I exited the loop and updated the sample buffer’s pixel
			value for the location to the average Vector3D and set sampleCountBuffer for the location to the actual
			number of samples used before converging.
		</p>
		<ul>
			<li>
				<p>
					Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image
					with
					clearly visible differences in sampling rate over various regions and pixels. Include both your
					sample
					rate image, which shows your how your adaptive sampling changes depending on which part of the image
					you
					are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max
					ray
					depth.
				</p>
			</li>
		</ul>

		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/pt5bunny.png" width="480px" />
						<figcaption align="middle">Bunny</figcaption>
				</tr>
			</table>
		</div>
		<div align="center">
			<table style="width=100%">
				<tr>
					<td align="middle">
						<img src="images/pt5bunny_rate.png" width="480px" />
						<figcaption align="middle">Bunny Rate</figcaption>
				</tr>
			</table>
		</div>



		<a
			href="https://cal-cs184-student.github.io/sp22-project-webpages-LisaCheung/proj3-1/index.html">https://cal-cs184-student.github.io/sp22-project-webpages-LisaCheung/proj3-1/index.html</a>
	</div>

</body>

</html>